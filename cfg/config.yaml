
api_keys:
    palm: AIzaSyBb8GFD5qPUtZ62zaZ3a2Vh8GMgoRg0UnI

datasets:
  all_sides: 
    type: "json"
    path: "{{project_root}}/data/all_sides/test.json"
    aliases:
      Left: doc1
      Right: doc2
    references: 
      - "Ahmed_Intersection"
      - "Naman_Intersection"
      - "Helen_Intersection" 
      - "AllSides_Intersection"
  privacy_policy: 
    type: "csv"
    path: "{{project_root}}/data/privacy_policy/3p_data.csv"
    aliases:
      Company_1: doc1
      Company_2: doc2
    references: 
      - "Annotator1"
      - "Annotator2"
      - "Annotator3"

resp_coll:
  ds_cache: "{{project_root}}/data/cache"
  save_dir: "{{project_root}}/data/results"
  num_attempts: 3
  checkpoint_interval: 50
  prompt_templates: 
    sys: "{{project_root}}/cfg/prompts/system.yaml"
    icl: "{{project_root}}/cfg/prompts/in_context_learning.yaml"
    l0: "{{project_root}}/cfg/prompts/teler_l0.yaml"
    l1: "{{project_root}}/cfg/prompts/teler_l1.yaml"
    l2: "{{project_root}}/cfg/prompts/teler_l2.yaml"
    l3: "{{project_root}}/cfg/prompts/teler_l3.yaml"
    l4: "{{project_root}}/cfg/prompts/teler_l4.yaml"
  models:
    - microsoft/Phi-3-mini-4k-instruct
    - microsoft/Phi-3-mini-128k-instruct
    - meta-llama/Meta-Llama-3-8B-Instruct
    - meta-llama/Llama-2-7b-chat-hf
    - meta-llama/Llama-2-13b-chat-hf
#    - "gemini-pro"
#    - "gpt-3.5-turbo"
#    - "gpt-4"
#    - "mistralai/Mistral-7B-Instruct-v0.1"
#    - "mistralai/Mistral-7B-Instruct-v0.2"
#    - "lmsys/vicuna-7b-v1.5"
#    - "lmsys/vicuna-7b-v1.5-16k"
#    - "lmsys/vicuna-13b-v1.5"
#    - "lmsys/vicuna-13b-v1.5-16k"
#    - "mosaicml/mpt-7b-instruct"
#    - "mosaicml/mpt-30b-instruct"
#    - "mosaicml/mpt-7b-chat"
#    - "mosaicml/mpt-30b-chat"
#    #- "tiiuae/falcon-7b-instruct"
#    #- "chat-bison-001"
#    #- "meta-llama/Llama-2-70b-chat-hf"

eval:
  metrics:
    - "rouge"
    #- "semf1"
    #- "bertscore"
    #- "fans"
  save_dir: "{{project_root}}/data/results/"

model_params:
  model_cache: "{{project_root}}/data/llm_cache"
  #model_cache: "/data/shared/llm_cache"
  #model_cache: "/data/john/cache"
  use_sampling_params: False
  max_length: 4096
  #max_length: 2048
  num_output_tokens: 512
  temperature: 0.8
  batch_size: 2
  num_devices: 4
  #dtype: "float16"
  dtype: "auto"

unit_test: 
  cases:
    #- "eval"
    - "llm-responses"
    #- "teler"

