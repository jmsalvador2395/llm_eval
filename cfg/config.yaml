datasets:
  all_sides: 
    type: "json"
    path: "{{project_root}}/data/ds/all_sides/test.json"
    aliases:
      Left: doc1
      Right: doc2
    references: 
      - "Ahmed_Intersection"
      - "Naman_Intersection"
      - "Helen_Intersection" 
      - "AllSides_Intersection"
  privacy_policy: 
    type: "csv"
    path: "{{project_root}}/data/ds/privacy_policy/3p_data.csv"
    aliases:
      Company_1: doc1
      Company_2: doc2
    references: 
      - "Annotator1"
      - "Annotator2"
      - "Annotator3"
  stanfordnlp/sst2:
    text_column: "sentence"
    split: "test"
  abisee/cnn_dailymail:
    text_column: "article"
    split: "test"
    args:
      - "1.0.0"
    max_tokens: 2048
  sind:
    path: "{{project_root}}/data/ds/sind"
    file_map:
      train: "train.tsv"
      dev: "dev.tsv"
      test: "test.tsv"
    text_column: "text"
    split:
      - "test"
      - "dev"
  roc:
    path: "{{project_root}}/data/ds/ROCStories"
    text_column: "text"
    split: 'test'
  wikipedia:
    test_size: 10000
    split:
      - "test"
    text_column: "text"
    args:
      - "20231101.en"
    min_sents: 10
    max_tokens: 2048

    
  acl_abstracts:
    type: "json"
    path: "{{project_root}}/data/ACL_titles_abstracts_dataset/acl_abstracts.json"
    text_column: "text"

infill:
  seed: 100
  ds_cache: "{{project_root}}/data/cache"
  save_dir: "{{project_root}}/data/results"
  target_data:
    - "roc"
    - "sind"
    - "abisee/cnn_dailymail"
    - "wikipedia"
  split_tokens:
    roc: "<eos>"
    sind: "<eos>"
  num_proc: 8
  chunk_size: 32
  prompt_templates:
    # sys: "{{project_root}}/cfg/prompts/fitb_system.yaml"
    # fitb_l0: "{{project_root}}/cfg/prompts/fitb_l0.yaml" 
    # fitb_l1: "{{project_root}}/cfg/prompts/fitb_l1.yaml" 
    # fitb_l2: "{{project_root}}/cfg/prompts/fitb_l2.yaml" 
    # fitb_l3: "{{project_root}}/cfg/prompts/fitb_l3.yaml" 
    # fitb_l4: "{{project_root}}/cfg/prompts/fitb_l4.yaml" 
    sys: "{{project_root}}/cfg/prompts/fitb_format/fitb_system.yaml"
    fitb_l0: "{{project_root}}/cfg/prompts/fitb_format/fitb_l0.yaml" 
    fitb_l1: "{{project_root}}/cfg/prompts/fitb_format/fitb_l1.yaml" 
    fitb_l2: "{{project_root}}/cfg/prompts/fitb_format/fitb_l2.yaml" 
    fitb_l3: "{{project_root}}/cfg/prompts/fitb_format/fitb_l3.yaml" 
    fitb_l4: "{{project_root}}/cfg/prompts/fitb_format/fitb_l4.yaml" 
  limit_tokenizers: 
    - meta-llama/Llama-3.2-1B-Instruct
    - google/gemma-2-2b-it
    - allenai/OLMo-2-1124-7B-Instruct
    - mistralai/Mistral-Nemo-Instruct-2407
  max_blank_sents: 3
  max_blank_words: 0
  one_of_each: True
  kwargs:
    bertscore:
      lang: "en"
      nthreads: 16
      batch_size: 200
      rescale_with_baseline: True
    rouge:
      use_aggregator: False
    perplexity:
      model_id: "meta-llama/Llama-3.2-1B"
      batch_size: 4
  temps:
    #- 0.7
    #- 0.5
    - 0.3
  picked_templates:
    meta-llama/Llama-3.2-1B-Instruct:
      fitb_l0:
        tids: [0, 0]
        sids: [5, 2]
      fitb_l1:
        tids: [2, 3]
        sids: [5, 2]
      fitb_l2:
        tids: [1, 2]
        sids: [5, 1]
      fitb_l3:
        tids: [1, 2]
        sids: [5, 2]
      fitb_l4:
        tids: [5, 1]
        sids: [5, 2]
  ans_extract_patterns:
    - |
      <ANSWER>(.*?)</ANSWER>

resp_coll:
  ds_cache: "{{project_root}}/data/cache"
  save_dir: "{{project_root}}/data/results"
  num_attempts: 3
  checkpoint_interval: 50
  prompt_templates: 
    sys: "{{project_root}}/cfg/prompts/system.yaml"
    icl: "{{project_root}}/cfg/prompts/in_context_learning.yaml"
    l0: "{{project_root}}/cfg/prompts/teler_l0.yaml"
    l1: "{{project_root}}/cfg/prompts/teler_l1.yaml"
    l2: "{{project_root}}/cfg/prompts/teler_l2.yaml"
    l3: "{{project_root}}/cfg/prompts/teler_l3.yaml"
    l4: "{{project_root}}/cfg/prompts/teler_l4.yaml"
  models:
    - microsoft/Phi-3-mini-4k-instruct
    - microsoft/Phi-3-mini-128k-instruct
    - meta-llama/Meta-Llama-3-8B-Instruct
    - meta-llama/Llama-2-7b-chat-hf
    - meta-llama/Llama-2-13b-chat-hf
    - meta-llama/Llama-3.2-1B-Instruct

eval:
  metrics:
    - "rouge"
    #- "semf1"
    #- "bertscore"
    #- "fans"
  save_dir: "{{project_root}}/data/results/"

model_params:
  model_cache: "{{project_root}}/data/llm_cache"
  default:
    max_model_len: 4096
    trust_remote_code: True
    tensor_parallel_size: 2
  meta-llama/Llama-3.1-8B-Instruct:
    max_model_len: 4096
  meta-llama/Llama-3.2-1B-Instruct:
    max_model_len: 4096
  meta-llama/Llama-3.2-3B-Instruct:
    max_model_len: 4096
  meta-llama/Llama-3.3-70B-Instruct:
    max_model_len: 4096
    tensor_parallel_size: 2
    quantization: "bitsandbytes"
    load_format: "bitsandbytes"
    trust_remote_code: True
  deepseek-ai/DeepSeek-R1-Distill-Llama-70B:
    max_model_len: 4096
    tensor_parallel_size: 2
    quantization: "bitsandbytes"
    load_format: "bitsandbytes"
    trust_remote_code: True

unit_test: 
  cases:
    #- "eval"
    - "llm-responses"
    #- "teler"

