#!/usr/bin/env python

import os
import argparse
import sqlite3
import numpy as np
import time
from datasets import Dataset
from pprint import pprint
from itertools import product
from tabulate import tabulate
from tqdm import tqdm
from vllm import LLM, SamplingParams

from IPython.terminal.embed import InteractiveShellEmbed

def main(args):
    model = args.model
    db_path = args.path

    con = sqlite3.connect(db_path)
    cur = con.cursor()
    tables = list(list(zip(*cur.execute(
        'select name from sqlite_master where type="table";'
    ).fetchall()))[0])
    try:
        views = list(list(zip(*cur.execute(
            'select name from sqlite_master where type="view";'
        ).fetchall()))[0])
    except:
        views = []

    columns = dict()
    for name in tables:
        tbl_info = cur.execute(f"PRAGMA table_info({name});").fetchall()
        _, cols, _, _, _, _ = zip(*tbl_info)
        columns[name] = list(cols)
    for name in views:
        vw_info = cur.execute(f"PRAGMA table_info({name});").fetchall()
        _, cols, _, _, _, _ = zip(*vw_info)
        columns[name] = list(cols)
    
    kwargs = {
        "meta-llama/Llama-3.1-8B-Instruct": {
            "tensor_parallel_size": 2,
        },
        "meta-llama/Llama-3.2-1B-Instruct": {
            "tensor_parallel_size": 2,
        },
        "meta-llama/Llama-3.2-3B-Instruct": {
            "tensor_parallel_size": 2,
        },
        "meta-llama/Llama-3.3-70B-Instruct": {
            "tensor_parallel_size": 2,
            "quantization": "bitsandbytes",
            "load_format": "bitsandbytes",
            "trust_remote_code": True,
        },
        "mistralai/Mistral-Large-Instruct-2411": {
            "tensor_parallel_size": 2,
            "quantization": "bitsandbytes",
            "load_format": "bitsandbytes",
            "trust_remote_code": True,
        }
    }
    default = {
        "tensor_parallel_size": 2,
    }
    # load model
    keys = columns['prompts']
    llm = LLM(
        model, 
        download_dir=args.cache,
        enforce_eager=True,
        worker_use_ray=True,
        max_model_len=50_000,
        **kwargs.get(model, default),
    )

    sampling_params = SamplingParams(
        temperature=0.3,
        top_p=1.0,
        top_k=-1,
        seed=int(time.time()),
        max_tokens=50_000,
    )

    responses = []
    start = time.time()
    for prompts in tqdm(fetch_batches(
        cur, args.batch_size, args.limit)
    ):
        resp = llm.chat(prompts, sampling_params, use_tqdm=False)
    stop = time.time()

    print(f'*********************************************')
    print(
        f'model: {args.model}, limit: {args.limit:,}, '
        f'batch size: {args.batch_size} '
        f'num_devices: {2}, '
        f'time: {stop-start:,.02f}'
    )
    print(f'*********************************************')

    if args.interactive:
        ipshell = InteractiveShellEmbed()
        ipshell()

def fetch_batches(cur, batch_size, limit):
    read_cur =  cur.execute('select sys_text, prompt_text from prompts')
    prog = 0
    while prog < limit:
        sys, prompt = zip(*read_cur.fetchmany(batch_size))
        if not sys:  # No more rows to fetch
            break
        prompts = [
            [{'role': 'system', 'content': s},
             {'role': 'user', 'content': p},]
             for s, p in zip(sys, prompt)
        ]
        prog += len(prompts)
        yield prompts


if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '-m', '--model', type=str, required=True
    )
    parser.add_argument(
        '-p', '--path', type=str, required=True
    )
    parser.add_argument(
        '-c', '--cache', type=str, required=True
    )
    parser.add_argument(
        '-i', '--interactive', default=False, action='store_true'
    )
    parser.add_argument(
        '-l', '--limit', type=int, default=1000, 
    )
    parser.add_argument(
        '-b', '--batch_size', type=int, default=1000, 
    )

    args = parser.parse_args()
    main(args)