#!/usr/bin/env python

import argparse
import sqlite3
import numpy as np
import time
import json
import shutil
import os
import re
from difflib import ndiff
from tqdm import tqdm
from transformers import AutoTokenizer
from datasets import Dataset

from pprint import pprint
from IPython.terminal.embed import InteractiveShellEmbed

def main(args):
    pth = args.path
    out_name = args.out_name

    con = sqlite3.connect(pth)
    cur = con.cursor()
    tables = list(list(zip(*cur.execute(
        'select name from sqlite_master where type="table";'
    ).fetchall()))[0])

    columns = dict()
    for name in tables:
        tbl_info = cur.execute(f"PRAGMA table_info({name});").fetchall()
        _, cols, _, _, _, _ = zip(*tbl_info)
        columns[name] = list(cols)
    dsnames, = zip(
        *cur.execute(f'select distinct(dataset) from source_data')
    )

    #create a view for the data
    query = (
        f"""
        CREATE VIEW IF NOT EXISTS subset AS
        SELECT 
            R.rowid AS resp_id, R.model, P.template_name, 
            P.problem_id, P.template_id, P.sys_id, F.problem, F.answer, 
            P.prompt_text, R.response
        FROM responses R
        JOIN prompts P on R.prompt_id=P.rowid
        JOIN fitb_problems F on P.problem_id=F.rowid
        JOIN source_data C on F.ref_id=C.rowid
        """
    )
    keys = [
        'resp_id', 'model', 'template_name', 'problem_id', 
        'template_id', 'sys_id', 'problem', 'answer', 'prompt_text', 
        'response',
    ]
    res = cur.execute(query)
    con.commit()

    streamcur = con.cursor()
    N, = cur.execute(f'select count(*) from subset').fetchone()
    datastream = streamcur.execute(f'select {", ".join(keys)} from subset')
    metric_names, = zip(*cur.execute(
        f'select name from metric_names'
    ))
    out_data = []
    for sample in tqdm(
        cur_gen(datastream, batch_size=1, keys=keys),
        total=N
    ):
        resp_id = sample['resp_id']
        try:
            tags, extr_text = zip(*cur.execute(
                f'SELECT tag, text FROM extractions WHERE resp_id={resp_id}'
            ))
        except Exception as e:
            tags, extr_text = [], []
        sample['extractions'] = json.dumps(
            dict(zip(tags, extr_text)), indent=4
        )

        for metric in metric_names:
            try:
                tags, scores = zip(*cur.execute(
                    f"""
                    SELECT E.tag, M.score
                    FROM extractions E, {metric} M
                    WHERE E.rowid=M.extraction_id
                        AND E.resp_id={resp_id}
                    """
                ))
            except Exception as e:
                tags, scores = [], []
            sample[metric] = json.dumps(
                dict(zip(tags, scores)), indent=4
            )
        
        out_data.append(sample)
    ds = Dataset.from_list(out_data)
    models, = zip(*cur.execute(
        f'SELECT DISTINCT(model) FROM responses'
    ))

    patterns = [
        r'<ANSWER>(.*?)</ANSWER>',
        r'<ANSWER>(.*?)<ANSWER>',
    ]

    counts = count_nonempty(ds, models)
    fltr = ds.filter(lambda x: x['model'] == "meta-llama/Llama-3.2-1B-Instruct")
    el = 0
    test = extract_answers(fltr[el], patterns=patterns)
    ex = json.loads(fltr[el]['extractions'])

    ipshell = InteractiveShellEmbed()
    ipshell()

def extract_answers(
        sample: dict, 
        patterns=[], 
        blank_str='______',
        return_diffs=False
    ):
    response = sample['response']
    #extr = {'full': response}
    extr = dict()

    tags, texts = ['full'], [response]

    # convert to list if patterns is just a string
    if isinstance(patterns, str):
        patterns = [patterns]

    # extract using search patterns
    match_keys = []
    match_vals = []
    for i, pattern in enumerate(patterns):
        matches = re.findall(pattern, response)
        match_keys += [
            f'pattern:{i},pos:{k}' for k in range(len(matches))
        ]
        match_vals += matches
        # extr.update({
        #     f'pattern:{i},pos:{k}': match.strip()
        #     for k, match in enumerate(matches)
        # })
    tags += match_keys
    texts += match_vals

    diffs = list(ndiff(
        sample['problem'].split(), 
        response.split()
    ))

    # A = problem text, B = response text
    A_only = [word[2:] for word in diffs if word.startswith('- ')]
    B = [
        word[2:] for word in diffs 
        if re.match('^\+\s|^\s\s', word)
    ]
    B_only = [word[2:] for word in diffs if word.startswith('+ ')]

    tags.append('diff_seqs:all')
    texts.append(' '.join(B_only))

    # find uninterupted exclusive sub-sequences in text
    seqs = []
    candidate = ''
    for word in response.split():
        if word in B_only:
            candidate += f' {word}'
        elif candidate != '':
            seqs.append(candidate.strip())
            candidate = ''
    if candidate != '':
        seqs.append(candidate.strip())
    seqs = list(filter(lambda x: len(x.split()) > 3, seqs))

    tags += [f'diff_seqs:{i}' for i in range(len(seqs))]
    texts += seqs
    #extr.update({f'diff_seqs:{i}': seq for i, seq in enumerate(seqs)})
    lines = response.split('\n')
    lines = [line for line in lines if line != '']
    if len(lines) > 1:
        tags += [f'lines:{i}' for i in range(len(lines))]
        texts += lines

    # Matches numbered lists (e.g., "1. Item", "2) Item", "(3) Item")
    numbered_list_pattern = r'(?m)^\s*(?:\d+[\).]|\(\d+\))\s+(.+)$'
    
    # Matches bullet points (e.g., "- Item", "* Item", "• Item")
    bullet_list_pattern = r'(?m)^\s*[-•*]\s+(.+)$'
    
    matches = (
        re.findall(numbered_list_pattern, response) 
        + re.findall(bullet_list_pattern, response)
    )
    list_elements = [item.strip() for item in matches]
    tags += [f'list_el:{i}' for i in range(len(list_elements))]
    texts += list_elements

    matches = re.findall(r':\s*(.+)', response)
    tags += [f'post_colon:{i}' for i in range(len(matches))]
    texts += [match.strip() for match in matches]

    # Use left context of problem and right context of problem to match
    # the blank
    left_half, right_half = sample['problem'].split('______')
    matches = re.findall(f'{left_half}(.*?){right_half}', response)
    tags += [f'context_match:{i}' for i in range(len(matches))]
    texts += [match.strip() for match in matches]


    # filter out empty strings
    # extr = {k: v for k, v in extr.items() if v != ""}

    ##############################
    # filter and reducction step #
    ##############################

    # instantiate text_set with strings we already know to filter out
    invalid_set = {
        '', response, blank_str,
        left_half.strip() + ' ' + right_half.strip(),
    }
    text_tag_map = {
        text: [] for text in texts if text not in invalid_set
    }
    for tag, text in zip(tags, texts):
        if text not in invalid_set:
            text_tag_map[text].append(tag)
    extr = {
        ','.join(tag_set): text 
        for text, tag_set 
        in text_tag_map.items()
    }

    if return_diffs:
        return extr, diffs
    else:
        return extr

def count_nonempty(ds, models):
    num_nonempty = dict()
    for model in tqdm(models, position=0):
        fltr = ds.filter(lambda x: x['model'] == model)
        total = 0
        for sample in tqdm(fltr, position=1):
            ext = json.loads(sample['extractions'])
            if len(ext) > 0:
                total += 1
        num_nonempty[model] = total
    return num_nonempty
        

def get_counts(cur):

    models = (
        'meta-llama/Llama-3.1-8B-Instruct',
        'meta-llama/Llama-3.2-1B-Instruct',
        'meta-llama/Llama-3.2-3B-Instruct',
    )

    prompt_stats, prompt_lengths = count_prompt_tokens(cur)

    resp_attr, lengths, lengths_no_ast, resp_lengths = (
        count_response_tokens(cur, 1000)
    )

    cutoffs = [20_000, 25_000, 30_000]
    info = dict()
    for model in models:
        info[model] = {
            cutoff: np.sum(lengths[model] >= cutoff)
            for cutoff in cutoffs
        }
    return (
        prompt_stats, prompt_lengths, resp_attr, lengths, resp_lengths,
    )

def count_prompt_tokens(cur, batch_size=1000):

    tok = AutoTokenizer.from_pretrained(
        'meta-llama/Llama-3.1-8B-Instruct'
    )
    template_names, = zip(*cur.execute(
        f'SELECT DISTINCT(template_name) FROM prompts'
    ))
    prompt_stats = dict()
    lengths = dict()
    for tmplt in tqdm(template_names, position=0):
        prompt_stats[tmplt] = dict()
        lengths[tmplt] = []
        N, = cur.execute(
            f"""
            SELECT COUNT(*) FROM prompts P 
            WHERE template_name="{tmplt}"
            """
        ).fetchone()
        N_batch = N // batch_size
        if N % batch_size:
            N_batch += 1
        read_cur = cur.execute(
            f"""
            SELECT P.sys_text, P.prompt_text FROM Prompts P 
            WHERE template_name="{tmplt}"
            """
        )
        keys = ['sys_text', 'prompt_text']
        for batch in tqdm(
            cur_gen(read_cur, batch_size), total=N_batch, 
            position=1, leave=False,
        ):
            # format chat
            prompts = [
                [{'role': 'system', 'content': sys_text},
                 {'role': 'user', 'content': prompt_text}]
                for sys_text, prompt_text in batch
            ]

            # convert to tokens
            tokens = tok.apply_chat_template(prompts)

            # compute lengths
            lengths[tmplt] += list(map(len, tokens))

        lengths[tmplt] = np.array(lengths[tmplt])
        prompt_stats[tmplt] = {
            'max': int(np.max(lengths[tmplt])),
            'min': int(np.min(lengths[tmplt])),
            'mean': float(np.mean(lengths[tmplt])),
        }
    print(f'******** TOKEN COUNTS ********')
    print(json.dumps(prompt_stats, indent=4))
    return prompt_stats, lengths

def count_response_tokens(cur, batch_size=1000):
    models, = zip(*cur.execute(
        f'SELECT DISTINCT(model) FROM responses'
    ).fetchall())
    resp_attr = {model: dict() for model in models}
    lengths = {model: [] for model in models}
    lengths_no_ast = {model: [] for model in models}
    resp_lengths = {model: [] for model in models}
    for model in tqdm(models, position=0):

        tok = AutoTokenizer.from_pretrained(model)
        N, = cur.execute(
            f"""
            SELECT COUNT(*) FROM responses R, prompts P
            WHERE P.rowid=R.prompt_id and R.model="{model}"
            """
        ).fetchone()
        N_batch = N // batch_size
        if N % batch_size:
            N_batch += 1
        read_cur = cur.execute(
            f"""
            SELECT P.sys_text, P.prompt_text, R.response 
            FROM Prompts P, responses R 
            WHERE P.rowid=R.prompt_id and R.model="{model}"
            """
        )
        keys = ['sys_text', 'prompt_text', 'response']
        for batch in tqdm(
            cur_gen(read_cur, batch_size), total=N_batch, 
            position=1, leave=False,
        ):
            sys_text, prompt_text, response = zip(*batch)
            # format chat
            chats = [
                [{'role': 'system', 'content': sys_text},
                 {'role': 'user', 'content': prompt_text},
                 {'role': 'assistant', 'content': response}]
                 for sys_text, prompt_text, response in batch
            ]
            chats_no_ast = [
                [{'role': 'system', 'content': sys_text},
                 {'role': 'user', 'content': prompt_text}]
                 for sys_text, prompt_text, response in batch
            ]

            # convert to tokens
            tokens = tok.apply_chat_template(chats)
            tokens_no_ast = tok.apply_chat_template(chats_no_ast)
            resp_tokens = tok(
                response, add_special_tokens=False
            )['input_ids']

            # compute lengths
            lengths[model] += list(map(len, tokens))
            lengths_no_ast[model] += list(map(len, tokens_no_ast))
            resp_lengths[model] += list(map(len, resp_tokens))

        lengths[model] = np.array(lengths[model])
        lengths_no_ast[model] = np.array(lengths_no_ast[model])
        resp_lengths[model] = np.array(resp_lengths[model])
        resp_attr[model] = {
            'max': int(np.max(lengths[model])),
            'max_no_ast': int(np.max(lengths_no_ast[model])),
            'min': int(np.min(lengths[model])),
            'min_no_ast': int(np.min(lengths_no_ast[model])),
            'mean': float(np.mean(lengths[model])),
            'mean_no_ast': float(np.mean(lengths_no_ast[model])),
            'response_max': int(np.max(resp_lengths[model])),
            'response_min': int(np.min(resp_lengths[model])),
            'response_mean': float(np.mean(resp_lengths[model])),
        }
    print(f'******** TOKEN COUNTS ********')
    print(json.dumps(resp_attr, indent=4))
    return resp_attr, lengths, lengths_no_ast, resp_lengths
        

def cur_gen(cur, batch_size=1000, keys=None, list_of_dicts=False):
    while True:
        if batch_size == 1:
            sample = cur.fetchone()
            if not sample:
                break
            if keys:
                yield dict(zip(keys, sample))
            else:
                yield sample
        else:
            batch = cur.fetchmany(batch_size)
            if not batch:  # No more rows to fetch
                break
            if keys:
                if list_of_dicts:
                    yield [
                        dict(zip(keys, values)) for values in batch 
                    ]
                else:
                    yield dict(zip(keys, zip(*batch)))
            else:
                yield batch
if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('path', type=str)
    parser.add_argument(
        '-o', '--out_name', default='subset.db'
    )
    parser.add_argument(
        '-p', '--probs_per_ds', default=3
    )
    args = parser.parse_args()
    main(args)