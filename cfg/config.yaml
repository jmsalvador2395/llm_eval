datasets:
  all_sides: 
    type: "json"
    path: "{{project_root}}/data/all_sides/test.json"
    aliases:
      Left: doc1
      Right: doc2
    references: 
      - "Ahmed_Intersection"
      - "Naman_Intersection"
      - "Helen_Intersection" 
      - "AllSides_Intersection"
  privacy_policy: 
    type: "csv"
    path: "{{project_root}}/data/privacy_policy/3p_data.csv"
    aliases:
      Company_1: doc1
      Company_2: doc2
    references: 
      - "Annotator1"
      - "Annotator2"
      - "Annotator3"
  stanfordnlp/sst2:
    text_column: "sentence"
    split: "test"
  abisee/cnn_dailymail:
    text_column: "article"
    split: "test"
    args:
      - "1.0.0"
    
  acl_abstracts:
    type: "json"
    path: "{{project_root}}/data/ACL_titles_abstracts_dataset/acl_abstracts.json"
    text_column: "text"

infill:
  ds_cache: "{{project_root}}/data/cache"
  save_dir: "{{project_root}}/data/results"
  batch_size: 1000
  prompt_templates:
  target_data:
    - "stanfordnlp/sst2"
    - "abisee/cnn_dailymail"
  num_proc: 8
  chunk_size: 32
  prompt_templates:
    sys: "{{project_root}}/cfg/prompts/fitb_system.yaml"
    fitb_l0: "{{project_root}}/cfg/prompts/fitb_l0.yaml" 
    fitb_l1: "{{project_root}}/cfg/prompts/fitb_l1.yaml" 
    fitb_l2: "{{project_root}}/cfg/prompts/fitb_l2.yaml" 
    fitb_l3: "{{project_root}}/cfg/prompts/fitb_l3.yaml" 
    fitb_l4: "{{project_root}}/cfg/prompts/fitb_l4.yaml" 
  max_blank_sents: 5
  max_blank_words: 0
  one_of_each: True
  kwargs:
    bertscore:
      lang: "en"
    rouge:
      use_aggregator: False

resp_coll:
  ds_cache: "{{project_root}}/data/cache"
  save_dir: "{{project_root}}/data/results"
  num_attempts: 3
  checkpoint_interval: 50
  prompt_templates: 
    sys: "{{project_root}}/cfg/prompts/system.yaml"
    icl: "{{project_root}}/cfg/prompts/in_context_learning.yaml"
    l0: "{{project_root}}/cfg/prompts/teler_l0.yaml"
    l1: "{{project_root}}/cfg/prompts/teler_l1.yaml"
    l2: "{{project_root}}/cfg/prompts/teler_l2.yaml"
    l3: "{{project_root}}/cfg/prompts/teler_l3.yaml"
    l4: "{{project_root}}/cfg/prompts/teler_l4.yaml"
  models:
    - microsoft/Phi-3-mini-4k-instruct
    - microsoft/Phi-3-mini-128k-instruct
    - meta-llama/Meta-Llama-3-8B-Instruct
    - meta-llama/Llama-2-7b-chat-hf
    - meta-llama/Llama-2-13b-chat-hf
    - meta-llama/Llama-3.2-1B-Instruct

eval:
  metrics:
    - "rouge"
    #- "semf1"
    #- "bertscore"
    #- "fans"
  save_dir: "{{project_root}}/data/results/"

model_params:
  model_cache: "{{project_root}}/data/llm_cache"
  max_length: 4096
  num_output_tokens: 512
  temperature: 0.2
  dtype: "auto"

unit_test: 
  cases:
    #- "eval"
    - "llm-responses"
    #- "teler"

