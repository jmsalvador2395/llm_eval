#!/usr/bin/env python

import os
import argparse
import sqlite3
import numpy as np
import re
import pandas as pd
import evaluate
from datasets import Dataset
from pprint import pprint
from itertools import product
from tabulate import tabulate
from tqdm import tqdm
from difflib import ndiff
from collections import Counter

from llm_eval import cfg_reader
from llm_eval.response_collector.infill import extract_answers
from llm_eval.utils import files
from llm_eval.response_collector import db_funcs

from IPython.terminal.embed import InteractiveShellEmbed

def main(args):
    pth = args.db_path
    batch_size = args.batch_size
    cfg, keywords = cfg_reader.load(args.cfg)
    con = sqlite3.connect(pth)
    cur = con.cursor()

    supported_metrics = {
        'rouge', 'bertscore', 'meteor', 'chrf',
        'bertscore-context', 'rouge-context',
        'chrf-context', 'meteor-context',
    }
    if args.metric not in supported_metrics:
        raise Exception("not implemented")

    trgt_metric = args.metric
    

    # add additional metrics here
    if trgt_metric in ['bertscore', 'bertscore-context']:
        met = evaluate.load('bertscore')
        metric_kwargs = {
            'lang': 'en',
            'rescale_with_baseline': True,
            'nthreads': 8,
            'batch_size': 250,
        }
    elif trgt_metric in ['rouge', 'rouge-context']:
        met = evaluate.load('rouge')
        metric_kwargs = {
            'use_aggregator': False,
        }
    elif trgt_metric in ['meteor', 'meteor-context']:
        met = evaluate.load('meteor')
        metric_kwargs = dict()
    elif trgt_metric in ['chrf', 'chrf-context']:
        met = evaluate.load('chrf')
        metric_kwargs = dict()
    else:
        raise Exception('not implemented')

    cur.execute(
        f"""
        CREATE TABLE IF NOT EXISTS scores (
            eid int,
            reference text,
            metric text,
            score double,
            PRIMARY KEY (eid, metric, reference)
            FOREIGN KEY (eid) REFERENCES extractions(rowid)
        )
        """
    )
    N, = cur.execute(f'SELECT count(*) FROM extractions').fetchone()
    completed, = cur.execute(
        f'SELECT count(*) FROM scores WHERE metric="{trgt_metric}"'
    ).fetchone()

    N_batch = N // batch_size
    if N % batch_size:
        N_batch += 1

    stream_cur = con.cursor()
    res = stream_cur.execute(
        f"""
        SELECT E.rowid, E.text as pred, F.answer, R.response, F.problem, R.model
        FROM responses R, prompts P, fitb_problems F, extractions E
        WHERE
            R.prompt_id=P.rowid
            AND P.problem_id=F.rowid
            AND E.resp_id=R.rowid
        """
    )

    data_stream = db_funcs.cur_gen(
        res, 
        batch_size=batch_size, 
        keys=['rowid', 'pred', 'answer', 'response', 'problem', 'model'],
        list_of_dicts=True,
    )

    for n, batch in tqdm(
        enumerate(data_stream), total=N_batch,
        desc=f'computing {trgt_metric}',
    ):
        if n*batch_size < completed:
            continue
        ids = [x['rowid'] for x in batch]
        refs = [x['answer'] for x in batch]
        preds = [x['pred'] for x in batch]
        if trgt_metric in ['meteor', 'chrf']:
            key = 'score' if 'chrf' in trgt_metric else 'meteor'
            scores = [
                met.compute(predictions=[pred], references=[ref])[key]
                for pred, ref, in zip(preds, refs)
            ]
        elif trgt_metric.endswith('-context'):
            left_context, right_context = zip(*[
                sample['problem'].split('______') 
                for sample in batch
            ])
            preds = [
                f"{lc.strip()} {pred.strip()} {rc.strip()}" 
                for lc, pred, rc 
                in zip(left_context, preds, right_context)
            ]
            refs = [
                f"{lc.strip()} {ref.strip()} {rc.strip()}" 
                for lc, ref, rc 
                in zip(left_context, refs, right_context)
            ]
            if trgt_metric in ['bertscore-context', 'rouge-context']:
                scores = met.compute(
                    predictions=preds, references=refs,
                    **metric_kwargs,
                )
                if trgt_metric == 'bertscore-context':
                    scores = scores['f1']
                else:
                    scores = scores['rouge1']
            elif trgt_metric in ['chrf-context', 'meteor-context']:
                key = 'score' if 'chrf' in trgt_metric else 'meteor'
                scores = [
                    met.compute(predictions=[pred], references=[ref])[key]
                    for pred, ref, in zip(preds, refs)
                ]
            else:
                raise Exception(f'invalid metric: {trgt_metric}')
        else:
            scores = met.compute(
                predictions=preds, references=refs,
                **metric_kwargs,
            )
            if trgt_metric == 'bertscore':
                scores = scores['f1']
            elif trgt_metric == 'rouge':
                scores = scores['rouge1']
            else:
                raise Exception(f'invalid metric: {trgt_metric}')
        
        cur.executemany(
            f"""
            INSERT INTO scores (eid, reference, metric, score)
            VALUES (?, ?, ?, ?)
            """,
            zip(ids, refs, [trgt_metric]*len(ids), scores)
        )
        con.commit()

    # ipshell = InteractiveShellEmbed()
    # ipshell()

def make_all_data_ds(cursor):
    keys = [
        'resp_id', 'model', 'temperature', 'template_name', 
        'problem_id', 'template_id', 'sys_id', 'problem', 'answer', 
        'prompt_text', 'response', 
    ]
    ds = make_ds(cursor.execute(f'select * from all_data'), keys)
    return ds

def cursor_generator(cursor, keys):
    for row in cursor:
        yield dict(zip(keys, row))
        
def make_ds(cursor, keys):
    return Dataset.from_dict(dict(zip(keys, zip(*cursor.fetchall()))))

def cur_gen(cur, batch_size=1000, keys=None):
    while True:
        batch = cur.fetchmany(batch_size)
        if not batch:  # No more rows to fetch
            break
        if keys:
            yield dict(zip(keys, zip(*batch)))
        else:
            yield batch

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument(
        'metric', type=str,
    )
    parser.add_argument(
        '--db_path', '-d', type=str,
        default=f'{files.project_root()}/data/results/infill/re_final_set/data.db',
    )
    parser.add_argument(
        '--cfg', '-c', type=str,
        default=f'{files.project_root()}/cfg/config.yaml',
    )
    parser.add_argument(
        '--batch_size', '-b', type=int,
        default=5000,
    )
    args = parser.parse_args()
    main(args)